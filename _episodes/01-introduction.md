---
title: "Introduction"
teaching: 0
exercises: 0
questions:
- "What is heterogeneous parallel programming? Where did it come from and how did it evolve?"
- "What are the main differences between CPU and GPU architectures and their corresponding parallel programming paradigms?"
- "What is CUDA? Why do I need to know about it?"
objectives:
- "First learning objective. (FIXME)"
keypoints:
- "First key point. Brief Answer to questions. (FIXME)"
---

## Background

**High-performance computing (HPC)** is a highly multi-disciplinary area of research
at the intersection of computing systems, hardware architectures, software platforms
and parallel programming paradigms. The main goal of HPC is to deliver high-troughput 
and efficient solutions to computationally expensive problems through simultaneous use
of multiple processing units and computational resources.
The invention of **graphics processing units (GPUs)** more than two decades 
ago by NVIDIA was followed by significant improvements in both GPU architecture and
software design. During this period, NVIDIA has introduced a new GPU architecture in
roughly every 2 years: Tesla (2007), Fermi (2009), Kepler (2012), Maxwell (2014),
Pascal (2016), Volta (2017), Turing (2018), and Ampere (2020). The aforementioned
GPU architectures are often part of the following production line families:

- **Tegra**: designed for mobile and embedded devices such as smart phones and tablets
- **Geforce and Titan**: built for consumer graphics and entertainment
- **Quadro**: created for professional visualization
- **Tesla**: optimized for technical and scientific computing
- **Jetson**: suitable for artificial intelligence (AI)-driven autonomous machines 

As detailed in the [Software/Hardware Specifications](#sh-specifications)
section, we will use two CUDA-enabled GPU devices from the GeForce family 
with Kepler and Turing microarchitectures throughout this tutorial.

## Parallel Programming Paradigms

A programmer might see a program as a construct consisting of
`data` and `instructions` interacting with them. In the absence
of *data dependency* between a set of instructions, *e.g.*, where 
the resulting data generated by an instruction from the first *task* is
not required for performing another instruction in a second task,
the sequential (serial) code can run as a *parallel program* and perform 
the aformentioned (data-)independent tasks, concurrently.
Therefore, two fundamental types of parallelism can be defined
for each program: (i) *task parallelism*, and (ii) *data parallelism*.
Task parallelism is based on distribution of independent instructions on 
multiple processing units mainly because some functionalities and instructions
in the code might be able to operate independently. Meanwhile, data parallelism
delocalizes the data across multiple processing units since some data 
within each task can be operated upon in parallel by multiple processors.

In order to write a parralel code, **homogeneous parallel programming**
is often adopted, in which one or more processing units of the same architecture type
perform the tasks concurrently. However, the **heterogeneous parallel programming**
offers a more rigorous alternative where processing units from multiple architecture
types are responsible for performing parallelization. Here, GPUs perform the
data-intensive tasks and **central processing units (CPUs)** perform the 
instruction-intensive operations in order to improve the overall performance
compared with both sequential and homogeneous parallel programming models.

To better understand the concept of heterogeneous parallel programming,
we should introduce the following new concepts:

- **Latency**: The duration of an operation from its beginning to its completion expressed in microseconds (*m*s)
- **Throughput**: The number of operations processed per unit of time expressed in gigaflops (*G*flops), which
translates into a billion floating-point operations per second.
- **Bandwidth**: The processed amount of data per unit of time expressed in megabytes per second (*M*B/s) 
or gigabytes per second (*G*B/s)

Now, let us compare the main architectural differences between CPUs and GPUs
in order to be able to see the motivation behind homogeneous parallel computing
in terms of the concepts we just learned, more clearly:

![Figure 1](../fig/CPU_GPU_comparison.png)

CPUs with heavy-weight threads are designed to reduce the latency

## CUDA: A Platform for Heterogeneous Parallel Programming

Since its first release in 2007, **compute unified device architecture (CUDA)**
has become the major standard platform for the **general-purpose
computation using GPU (GPGPU)**, a term coined by
[Mark Harris](https://developer.nvidia.com/blog/author/mharris) highlighting
the non-graphical computational applications performed using GPUs.


{% include links.md %}

